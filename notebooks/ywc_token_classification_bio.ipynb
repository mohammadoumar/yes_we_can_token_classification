{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "273141c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.12.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (1.15.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 7.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from seqeval) (1.21.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from seqeval) (1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=bb4ab0ced7f91b1581b0899514b390f9ee801fb69877a1ff9c5d5a0d8ca6d4ad\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yeml15_9/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting IProgress\n",
      "  Downloading IProgress-0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from IProgress) (1.16.0)\n",
      "Installing collected packages: IProgress\n",
      "Successfully installed IProgress-0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install seqeval\n",
    "!pip install ipywidgets\n",
    "!pip install IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0571cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2d88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Sequence, ClassLabel\n",
    "from datasets import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aca55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378db399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"conll2003\", revision=\"master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad3e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f603b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PE_DATA_FILE = '/notebooks/ywc_token_classification/ywc_TokCL.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f21f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_dataset = torch.load(PE_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "680beffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_speech', 'Text', 'Part', 'Document', 'Order', 'Sentence', 'Start', 'End', 'Annotator', 'Tag', 'Component', 'Speech', 'Speaker', 'SpeakerType', 'Set', 'Date', 'Year', 'Name', 'MainTag', 'full_speech_words', 'tags_list', 'multi_class_tags', 'bio_only_tags', 'posfull_tokens_list', 'full_speech_words_proper', 'is_equal_tag', 'mc_int_tags', 'bio_int_tags', 'is_equal_tag_four'],\n",
       "        num_rows: 1589\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_speech', 'Text', 'Part', 'Document', 'Order', 'Sentence', 'Start', 'End', 'Annotator', 'Tag', 'Component', 'Speech', 'Speaker', 'SpeakerType', 'Set', 'Date', 'Year', 'Name', 'MainTag', 'full_speech_words', 'tags_list', 'multi_class_tags', 'bio_only_tags', 'posfull_tokens_list', 'full_speech_words_proper', 'is_equal_tag', 'mc_int_tags', 'bio_int_tags', 'is_equal_tag_four'],\n",
       "        num_rows: 847\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['full_speech', 'Text', 'Part', 'Document', 'Order', 'Sentence', 'Start', 'End', 'Annotator', 'Tag', 'Component', 'Speech', 'Speaker', 'SpeakerType', 'Set', 'Date', 'Year', 'Name', 'MainTag', 'full_speech_words', 'tags_list', 'multi_class_tags', 'bio_only_tags', 'posfull_tokens_list', 'full_speech_words_proper', 'is_equal_tag', 'mc_int_tags', 'bio_int_tags', 'is_equal_tag_four'],\n",
       "        num_rows: 635\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b50867f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And for the state to interfere in that decision, under whatever guise, and with whatever rationale, for the state to try to take over in that situation, and by edict, command what the individual shall do, and substitute itself for that individual's conscience, for her right to consult her rabbi, her minister, her priest, her doctor - any other counselor of her choice - I think goes beyond what we want to ever see accomplished in this country, if we really believe in the First Amendment: if we really believe in freedom of choice and the right of the individual.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['train']['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd2046b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': Value(dtype='string', id=None),\n",
       " 'essay': Value(dtype='string', id=None),\n",
       " 'essay_nr': Value(dtype='string', id=None),\n",
       " 'para_nr': Value(dtype='int64', id=None),\n",
       " 'paragraph': Value(dtype='string', id=None),\n",
       " 'paragraph_words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'tags_list_int': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_int': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_correction': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbfdc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_dataset = pe_dataset.rename_column(\"tags_list_int\", \"paragraph_tags_list_int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d757f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': Value(dtype='string', id=None),\n",
       " 'essay': Value(dtype='string', id=None),\n",
       " 'essay_nr': Value(dtype='string', id=None),\n",
       " 'para_nr': Value(dtype='int64', id=None),\n",
       " 'paragraph': Value(dtype='string', id=None),\n",
       " 'paragraph_words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_list_int': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_int': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_correction': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ea149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_dataset = pe_dataset.rename_column(\"paragraph_tags_list_int\", \"paragraph_tags_int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e18ae880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': Value(dtype='string', id=None),\n",
       " 'essay': Value(dtype='string', id=None),\n",
       " 'essay_nr': Value(dtype='string', id=None),\n",
       " 'para_nr': Value(dtype='int64', id=None),\n",
       " 'paragraph': Value(dtype='string', id=None),\n",
       " 'paragraph_words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_int': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_int': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_correction': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2b60b",
   "metadata": {},
   "source": [
    "### my work today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69e80de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we sequence the int labels to the class labels? will it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a45f80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_dataset['train'].features['paragraph_tags_bio_only_int'] = Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None),\n",
    "                                             length=-1, id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1467583b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['train'].features['paragraph_tags_bio_only_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "581dde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_dataset['test'].features['paragraph_tags_bio_only_int'] = Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None),\n",
    "                                             length=-1, id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce86937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_dataset['validation'].features['paragraph_tags_bio_only_int'] = Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None),\n",
    "                                             length=-1, id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f0db5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['test'].features['paragraph_tags_bio_only_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf91fdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['validation'].features['paragraph_tags_bio_only_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff6b838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': Value(dtype='string', id=None),\n",
       " 'essay': Value(dtype='string', id=None),\n",
       " 'essay_nr': Value(dtype='string', id=None),\n",
       " 'para_nr': Value(dtype='int64', id=None),\n",
       " 'paragraph': Value(dtype='string', id=None),\n",
       " 'paragraph_words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_int': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_int': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None), length=-1, id=None),\n",
       " 'paragraph_tags_bio_only_correction': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8842ba63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset[\"train\"][0][\"paragraph_tags_bio_only_int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f164001d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=3, names=['O', 'B-COMP', 'I-COMP'], names_file=None, id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset[\"train\"].features[\"paragraph_tags_bio_only_int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "887eab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7311a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00f97a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aca0726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed3db2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels_hf(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"paragraph_words\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"paragraph_tags_bio_only_int\"]\n",
    "    #print(examples[\"paragraph_tags\"])\n",
    "    #all_labels = labels_object.str2int(examples[\"paragraph_tags\"])\n",
    "    \n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6899ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee110ec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_and_align_labels_hf at 0x7f39f6e95280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc641a7be2a54d73b3c2bedb2f982529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f3ed79b8914e46a6d43a571ce67cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccc8a4f4d2e46989bd01c246fac7f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = pe_dataset.map(tokenize_and_align_labels_hf, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52236e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = pe_dataset[\"train\"].features['paragraph_tags_bio_only_int'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49939771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-COMP', 'I-COMP']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c719ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-COMP', 'I-COMP']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_dataset[\"train\"].features[\"paragraph_tags_bio_only_int\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d6f4e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26e1be10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "324dd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=32,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=20,\n",
    "    logging_steps=20\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f903b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e2a7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bce4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "445331e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e985d688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running training *****\n",
      "  Num examples = 1091\n",
      "  Num Epochs = 32\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2208' max='2208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2208/2208 10:03, Epoch 32/32]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.476400</td>\n",
       "      <td>0.398681</td>\n",
       "      <td>0.457543</td>\n",
       "      <td>0.578728</td>\n",
       "      <td>0.511050</td>\n",
       "      <td>0.846112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>0.280519</td>\n",
       "      <td>0.481909</td>\n",
       "      <td>0.652763</td>\n",
       "      <td>0.554473</td>\n",
       "      <td>0.894787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.281366</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.665276</td>\n",
       "      <td>0.566859</td>\n",
       "      <td>0.891101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.216100</td>\n",
       "      <td>0.267111</td>\n",
       "      <td>0.572621</td>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.636069</td>\n",
       "      <td>0.903865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.165600</td>\n",
       "      <td>0.268322</td>\n",
       "      <td>0.613579</td>\n",
       "      <td>0.763295</td>\n",
       "      <td>0.680297</td>\n",
       "      <td>0.911056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.281775</td>\n",
       "      <td>0.625108</td>\n",
       "      <td>0.752868</td>\n",
       "      <td>0.683065</td>\n",
       "      <td>0.908584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.311038</td>\n",
       "      <td>0.608074</td>\n",
       "      <td>0.753910</td>\n",
       "      <td>0.673184</td>\n",
       "      <td>0.905798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.322226</td>\n",
       "      <td>0.651794</td>\n",
       "      <td>0.776851</td>\n",
       "      <td>0.708849</td>\n",
       "      <td>0.906337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.352625</td>\n",
       "      <td>0.638502</td>\n",
       "      <td>0.764338</td>\n",
       "      <td>0.695776</td>\n",
       "      <td>0.902292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.362328</td>\n",
       "      <td>0.646649</td>\n",
       "      <td>0.774765</td>\n",
       "      <td>0.704934</td>\n",
       "      <td>0.911371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.393349</td>\n",
       "      <td>0.645075</td>\n",
       "      <td>0.758081</td>\n",
       "      <td>0.697028</td>\n",
       "      <td>0.901753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.397538</td>\n",
       "      <td>0.669052</td>\n",
       "      <td>0.779979</td>\n",
       "      <td>0.720270</td>\n",
       "      <td>0.906382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.414404</td>\n",
       "      <td>0.688789</td>\n",
       "      <td>0.800834</td>\n",
       "      <td>0.740598</td>\n",
       "      <td>0.913708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.434538</td>\n",
       "      <td>0.665493</td>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.721718</td>\n",
       "      <td>0.910292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.468353</td>\n",
       "      <td>0.691812</td>\n",
       "      <td>0.784150</td>\n",
       "      <td>0.735093</td>\n",
       "      <td>0.904270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.442773</td>\n",
       "      <td>0.685841</td>\n",
       "      <td>0.808133</td>\n",
       "      <td>0.741982</td>\n",
       "      <td>0.914831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.488145</td>\n",
       "      <td>0.702048</td>\n",
       "      <td>0.786236</td>\n",
       "      <td>0.741761</td>\n",
       "      <td>0.906247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.496763</td>\n",
       "      <td>0.689312</td>\n",
       "      <td>0.793535</td>\n",
       "      <td>0.737761</td>\n",
       "      <td>0.911146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.488823</td>\n",
       "      <td>0.679376</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.742073</td>\n",
       "      <td>0.915551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.512277</td>\n",
       "      <td>0.701097</td>\n",
       "      <td>0.799791</td>\n",
       "      <td>0.747199</td>\n",
       "      <td>0.912899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.494610</td>\n",
       "      <td>0.691086</td>\n",
       "      <td>0.816475</td>\n",
       "      <td>0.748566</td>\n",
       "      <td>0.915730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.504325</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.809176</td>\n",
       "      <td>0.761157</td>\n",
       "      <td>0.913888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.695770</td>\n",
       "      <td>0.806048</td>\n",
       "      <td>0.746860</td>\n",
       "      <td>0.913933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.558143</td>\n",
       "      <td>0.715695</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>0.769527</td>\n",
       "      <td>0.911011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.543873</td>\n",
       "      <td>0.715079</td>\n",
       "      <td>0.806048</td>\n",
       "      <td>0.757843</td>\n",
       "      <td>0.914247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.541454</td>\n",
       "      <td>0.705616</td>\n",
       "      <td>0.812304</td>\n",
       "      <td>0.755211</td>\n",
       "      <td>0.914022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.545163</td>\n",
       "      <td>0.702970</td>\n",
       "      <td>0.814390</td>\n",
       "      <td>0.754589</td>\n",
       "      <td>0.914517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.549903</td>\n",
       "      <td>0.723206</td>\n",
       "      <td>0.809176</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.915101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.552764</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.813347</td>\n",
       "      <td>0.765081</td>\n",
       "      <td>0.914517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.550938</td>\n",
       "      <td>0.725254</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.915596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.549747</td>\n",
       "      <td>0.715982</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.763389</td>\n",
       "      <td>0.915236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.550419</td>\n",
       "      <td>0.716377</td>\n",
       "      <td>0.816475</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.915281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-ner/checkpoint-500\n",
      "Configuration saved in bert-base-uncased-finetuned-ner/checkpoint-500/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-ner/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-ner/checkpoint-1000\n",
      "Configuration saved in bert-base-uncased-finetuned-ner/checkpoint-1000/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-ner/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-ner/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-ner/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-ner/checkpoint-1500\n",
      "Configuration saved in bert-base-uncased-finetuned-ner/checkpoint-1500/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-ner/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-ner/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-ner/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-ner/checkpoint-2000\n",
      "Configuration saved in bert-base-uncased-finetuned-ner/checkpoint-2000/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-ner/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-ner/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-ner/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2208, training_loss=0.07422333201933382, metrics={'train_runtime': 604.1008, 'train_samples_per_second': 57.792, 'train_steps_per_second': 3.655, 'total_flos': 2593568807103006.0, 'train_loss': 0.07422333201933382, 'epoch': 32.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb29c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 273\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5504189729690552,\n",
       " 'eval_precision': 0.716376944190302,\n",
       " 'eval_recall': 0.8164754953076121,\n",
       " 'eval_f1': 0.763157894736842,\n",
       " 'eval_accuracy': 0.9152808988764045,\n",
       " 'eval_runtime': 1.8387,\n",
       " 'eval_samples_per_second': 148.472,\n",
       " 'eval_steps_per_second': 9.789,\n",
       " 'epoch': 32.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efa9dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 355\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'COMP': {'precision': 0.752346570397112,\n",
       "  'recall': 0.8296178343949044,\n",
       "  'f1': 0.7890950397576675,\n",
       "  'number': 1256},\n",
       " 'overall_precision': 0.752346570397112,\n",
       " 'overall_recall': 0.8296178343949044,\n",
       " 'overall_f1': 0.7890950397576675,\n",
       " 'overall_accuracy': 0.9260755965859606}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b155ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task: try to do only with the BIO. then see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92084c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: paragraph_tags_bio_only_correction, essay, paragraph, paragraph_tags_int, split, para_nr, essay_nr, paragraph_tags_bio_only_int, paragraph_words, paragraph_tags, paragraph_tags_bio_only.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1091\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'COMP': {'precision': 0.9914893617021276,\n",
       "  'recall': 0.9954606141522029,\n",
       "  'f1': 0.993471019320453,\n",
       "  'number': 3745},\n",
       " 'overall_precision': 0.9914893617021276,\n",
       " 'overall_recall': 0.9954606141522029,\n",
       " 'overall_f1': 0.993471019320453,\n",
       " 'overall_accuracy': 0.9994651393423443}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"train\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results_train = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04cf06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
